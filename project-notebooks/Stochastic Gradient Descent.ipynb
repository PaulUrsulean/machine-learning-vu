{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# make plots look betters\n",
    "plt.style.use('ggplot')\n",
    "# plt.rcParams['figure.figsize'] = (8,6)\n",
    "# plt.rcParams['figure.dpi'] = (300)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "comments = pd.read_pickle('../toxic_comment_data/combined_train_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we only did feature extraction on the Comments column of the data frame, so we could just pass that directly to fit(). Now however, we would like to pass the whole data frame so we can extract particular features from it. Thus, we first we have to write some code to extract the information we want. The canonical way of doing this is with transformer extractor methods that can be used as part of a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# allows us to select a column by name from a data frame and return it as a nparray of type string\n",
    "class TextExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "\n",
    "    def transform(self, data):\n",
    "        return np.asarray(data[self.column]).astype(str)\n",
    "    \n",
    "    # not used\n",
    "    def fit(self, *_):\n",
    "        return self \n",
    "\n",
    "# allows us to select a column by name from a data frame and return it as a nparray of the specified type\n",
    "class ColumnExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column, dtype):\n",
    "        self.dtype = dtype\n",
    "        self.column = column\n",
    "\n",
    "    def transform(self, data):\n",
    "        data = np.asarray(data[self.column]).astype(self.dtype)\n",
    "         \n",
    "        # note: reshaping is necessary because otherwise sklearn\n",
    "        # interprets 1-d array as a single sample\n",
    "        # reshapes from row vector to column vector\n",
    "        data = data.reshape(data.size, 1) \n",
    "        return data\n",
    "    \n",
    "    # not used\n",
    "    def fit(self, *_):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# these pipelines can be a mess so I tried to break it into manageable chunks\n",
    "\n",
    "# a separate pipeline can be created for each feature we extract\n",
    "# for one liners this isnt done, and instead the extractors are called in the feature union\n",
    "\n",
    "char_ngram_features = Pipeline([\n",
    "    ('extractor', TextExtractor('Comment')),\n",
    "    ('vectorizer', CountVectorizer(analyzer='char')),\n",
    "    ('tfidf', TfidfTransformer())\n",
    "])\n",
    "\n",
    "word_ngram_features = Pipeline([\n",
    "    ('extractor', TextExtractor('Comment')),\n",
    "    ('vectorizer', CountVectorizer(analyzer='word')),\n",
    "    ('tfidf', TfidfTransformer())\n",
    "])\n",
    "\n",
    "# the features combined\n",
    "features = FeatureUnion([\n",
    "    ('char_ngrams', char_ngram_features),\n",
    "    ('word_ngrams', word_ngram_features),\n",
    "    ('norm_length', ColumnExtractor('Norm_True_Length',float)),\n",
    "    ('weekday', ColumnExtractor('Weekday',int)),\n",
    "    ('day', ColumnExtractor('Day',int)),\n",
    "    ('month', ColumnExtractor('Month',int)),\n",
    "    ('year', ColumnExtractor('Year',int)),\n",
    "    ('hour', ColumnExtractor('Hour',int)),\n",
    "    ('minute', ColumnExtractor('Minute',int)),\n",
    "    ('second', ColumnExtractor('Second',int))\n",
    "])\n",
    "\n",
    "# set the classifier\n",
    "#--** this is where to change the classifier **--\n",
    "# The parameters are default values, if they are not given here we end up with ugly warnings later on\n",
    "classifier = SGDClassifier(max_iter=5, tol=None)\n",
    "\n",
    "# set the feature selection (these params can also be grid searched)\n",
    "select = SelectPercentile(score_func=chi2, percentile=25)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('union', features),\n",
    "    ('select', select),\n",
    "    ('classifier', classifier)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline.fit(comments, comments.Insult);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_pickle(\"../toxic_comment_data/labeled_test_set_with_features.csv\")\n",
    "predicted = pipeline.predict(test)\n",
    "np.mean(predicted == test.Insult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {'union__char_ngrams__vectorizer__ngram_range': [(1, 1), (1, 2)] # the range of the n-grams\n",
    "             }\n",
    "\n",
    "# the names are important, e.g. vect__ corresponds to the name given to the class in the pipeline,\n",
    "# while \"ngram_range\" is a parameter for the class, i.e. CountVectorizer\n",
    "\n",
    "gs_clf = GridSearchCV(pipeline, parameters, n_jobs=-1) # run the grid search on the chosen pipeline\n",
    "gs_clf = gs_clf.fit(comments, comments.Insult) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(gs_clf.best_score_)\n",
    "print(gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_pickle(\"../toxic_comment_data/labeled_test_set_with_features.csv\")\n",
    "predicted = gs_clf.predict(test)\n",
    "np.mean(predicted == test.Insult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing from above, we already fit a NB classifier, and saw its accuracy. We now plot the ROC curve:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot ROC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a function for plotting the roc\n",
    "def plot_roc(test_labels, predicted_labels):\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(test_labels,predicted_labels)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    \n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(false_positive_rate, true_positive_rate, 'b',\n",
    "    label='AUC = %0.2f'% roc_auc)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0,1],[0,1],'r--')\n",
    "    plt.xlim([-0.1,1.1])\n",
    "    plt.ylim([-0.1,1.1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate');\n",
    "    \n",
    "plot_roc(test.Insult, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty awful. Now try doing some grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "start = timer() # start timer so we can see how long it takes\n",
    "\n",
    "# IF YOU RUN THIS AS IS IT WILL TAKE AN HOUR OR TWO TO FINISH\n",
    "parameters = {\n",
    "                'union__char_ngrams__vectorizer__ngram_range': [(1, 1), (1, 2), (1,3)], # the range of the n-grams\n",
    "                'union__char_ngrams__vectorizer__stop_words': [None, 'english'], # remove stopwords or not\n",
    "                'union__char_ngrams__tfidf__use_idf': (True, False), # use idf or not\n",
    "                'union__word_ngrams__vectorizer__ngram_range': [(1, 1), (1, 2), (1,3)],\n",
    "                'union__word_ngrams__vectorizer__stop_words': [None, 'english'],\n",
    "                'union__word_ngrams__tfidf__use_idf': (True, False),\n",
    "                'classifier__alpha' : [0.1, 0.01, 0.001], # change alpha hyperparameter\n",
    "                'classifier__loss' : ['hinge', 'modified_huber'],\n",
    "                'classifier__penalty' : ['l2', 'l1', 'elasticnet'],\n",
    "                'classifier__max_iter': [500, 1000, 2000],\n",
    "                'classifier__tol' : [None, 1e-3],\n",
    "                'select__percentile' : [10,20,40] # percentage of best features to select\n",
    "             }\n",
    "\n",
    "# The names here are very important. the intuition is that they follow from the pipeline hierarchy.\n",
    "# For example, to access the ngram_range parameter for CountVectorizer, you have to follow the hierarchy\n",
    "# starting at the feature union, and use the corresponding names with two '__' between each.\n",
    "\n",
    "# So the format is: name__name__(...)__parameter\n",
    "# The last name on the string e.g. \"ngram_range\" is a parameter for the class, in this case, CountVectorizer\n",
    "\n",
    "gs_clf = GridSearchCV(pipeline, parameters, n_jobs=-1) # run the grid search on the chosen pipeline\n",
    "gs_clf = gs_clf.fit(comments, comments.Insult) \n",
    "\n",
    "end = timer()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Training time:\", round((end-start)/60/60,2), \"hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the best score found, and the parameters that go with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(gs_clf.best_score_)\n",
    "print(gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the new classifier does on accuracy: (noticeable improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_pickle(\"../toxic_comment_data/labeled_test_set_with_features.csv\")\n",
    "predicted = gs_clf.predict(test)\n",
    "np.mean(predicted == test.Insult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot new ROC: (noticeable improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_roc(test.Insult, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model took a while to train, so we may wish to save for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module '__main__' has no attribute 'TextExtractor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bd742e5c0e6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#joblib.dump(gs_clf, 'first_try_select_SGDClassifier.pkl');\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgs_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sgd.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    576\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mload_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36m_unpickle\u001b[0;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             warnings.warn(\"The file '%s' has been generated with a \"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1048\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36mload_global\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1336\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1338\u001b[0;31m         \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1339\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGLOBAL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_global\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36mfind_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m   1390\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_getattribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module '__main__' has no attribute 'TextExtractor'"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "#joblib.dump(gs_clf, 'first_try_select_SGDClassifier.pkl');\n",
    "gs_clf = joblib.load('sgd.pkl');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
