{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "comments = pd.read_csv(\"../toxic_comment_data/train.csv\")\n",
    "comments2 = pd.read_csv(\"../toxic_comment_data/test_with_solutions.csv\")\n",
    "comments2 = comments2.drop(columns=[\"Usage\"])\n",
    "comments = pd.concat([comments,comments2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options for dealing with missing data\n",
    "\n",
    "1. Remove feature with missing data. This is definitely the simplest option, however this is most likely not a good idea since the contestants did find correlations between the comment date and the target value.\n",
    "\n",
    "2. Remove instances (rows) with features missing. In this case we have to determine whether the data is missing uniformly, i.e. there is no correlation between the fact that a value is missing and some other feature. To do this, we graph the missing feature against another feature.\n",
    "\n",
    "3. Use a model with the target value as the missing data. This also requires that the missing data is missing uniformly, so that we can perform a random sampling of instances without missing data that will act as our test set. This would be my choice, however this also adds a bit of extra work.\n",
    "\n",
    "### IMPORTANT: Training data and test data have to coincide.\n",
    "    CONSEQUENCE: Whichever way we decide to deal with the missing data, we first have to make our changes accross all data, and only then divide it up into training and test data. Intuition says that we shouldn't mess with test data in any way that would not be reproducible in a production environment where the production data is not necessarily in our control.\n",
    "    \n",
    "**Below is an indication as to how many of our values are missing.**\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_null = comments['Date'].isnull()\n",
    "miss = is_null.sum() # number of missing values\n",
    "non = len(is_null) - miss # number of non-insulting comments\n",
    "values = [miss,non]\n",
    "\n",
    "labels = 'Missing', 'Non-missing'\n",
    "colors = ['lightcoral', 'lightskyblue']\n",
    "explode = (0.1, 0)  # explode 1st slice\n",
    "\n",
    "def make_autopct(values):\n",
    "    def my_autopct(pct):\n",
    "        total = sum(values)\n",
    "        val = int(round(pct*total/100.0))\n",
    "        return '{p:.2f}%  ({v:d})'.format(p=pct,v=val)\n",
    "    return my_autopct\n",
    "\n",
    "\n",
    "plt.title(\"Percent and magnitude of the number of comments that are missing\")\n",
    "plt.pie(values, explode=explode, labels=labels, colors=colors,\n",
    "        autopct=make_autopct(values), startangle=180);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below we check whether the instances with missing data are distributed evenly by graphing the frequency of missing data over the histogram of raw comment length.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_length(input):\n",
    "    if type(input) is str:\n",
    "        return len(bytes(input, 'ascii').decode('unicode-escape'))\n",
    "    else: \n",
    "        return -1 #arbitrary, should not ever happen with this dataset\n",
    "\n",
    "comments['Raw_Length']  = comments['Comment'].str.len()\n",
    "comments['True_Length'] = comments['Comment'].apply(lambda x: true_length(x))\n",
    "\n",
    "# comments = comments[comments['RawLength'] < 2500]\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "ax1.hist(comments.Raw_Length,color=\"black\",bins=100, label=\"Comment length frequency\", alpha=1)\n",
    "\n",
    "data = comments[comments['Date'].isnull() == True].Raw_Length.values\n",
    "y, binEdges = np.histogram(data, bins=100)\n",
    "bincenters = 0.5*(binEdges[1:] + binEdges[:-1])\n",
    "ax2.plot(bincenters, y, '-', label=\"Missing data frequency\", color=\"red\")\n",
    "\n",
    "ax1.set_xlabel('Length of comment in characters')\n",
    "ax1.set_ylabel('Number of occurences of length')\n",
    "ax2.set_ylabel('Number of occurences of missing date')\n",
    "plt.title('Comment length distribution vs missing date distribution');\n",
    "ax1.set_xlim([0, 2000])\n",
    "ax1.set_ylim([0, 5000])\n",
    "ax2.set_xlim([0, 2000])\n",
    "ax2.set_ylim([0, 700])\n",
    "\n",
    "ax1.legend(loc=1);\n",
    "ax2.legend(loc=5);\n",
    "             \n",
    "# Sorry for spaghetti code\n",
    "# Also, the centers for the line on the graph should align with the center of the histogram.\n",
    "# This is most likely due to how the bin edges are chosen based on the ranges of the data, which are different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curves have a similar shape, so an even distribution of missing data can be assumed. Usually we would test this with more features, however length is the only numerical one we have thus far, so this is the best we've got. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
